# main.py - FastAPI Main Application
from fastapi import FastAPI, HTTPException, Depends, Security
from fastapi.concurrency import asynccontextmanager
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import uvicorn
from datetime import datetime
import logging
import time
from config import settings

# Import logging configuration first
import logging_config

# Import custom modules
from document_processor import DocumentProcessor
from query_engine import QueryEngine
from vector_store import PineconeVectorStore
from database import DatabaseManager
from performance_monitor import performance_monitor, monitor_performance
import hashlib

# Get logger for this module
logger = logging.getLogger(__name__)
logger.info("Main application module loaded")

# Global cache for embeddings and results
embedding_cache = {}
result_cache = {}
ultra_fast_cache = {}  # Ultra-fast cache for <20ms responses
instant_cache = {}  # Instant cache for <20ms responses
response_cache = {}  # Complete response cache

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize system components on startup"""
    logger.info("ðŸš€ Starting LLM-Powered Query-Retrieval System...")
    try:
        await vector_store.initialize()
        await query_engine.initialize()
        await database_manager.initialize()
        logger.info("âœ… System initialization completed")
    except Exception as e:
        logger.error(f"âŒ System initialization failed: {str(e)}")
        raise

    yield  # app runs here

    # Shutdown logic
    logger.info("ðŸ”„ Shutting down system...")
    logger.info("âœ… System shutdown completed")


app = FastAPI(
    title="LLM-Powered Intelligent Query-Retrieval System",
    description="Advanced document processing and query system for insurance, legal, HR, and compliance domains",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Security
security = HTTPBearer()
VALID_TOKEN = settings.AUTH_TOKEN
def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    if credentials.credentials != VALID_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid authentication token")
    return credentials.credentials

# Pydantic models for request/response
class QueryRequest(BaseModel):
    documents: str = Field(..., description="URL or path to document(s)")
    questions: List[str] = Field(..., description="List of natural language queries")

class QueryResponse(BaseModel):
    answers: List[str]

# Initialize system components
document_processor = DocumentProcessor()
vector_store = PineconeVectorStore()
query_engine = QueryEngine()
database_manager = DatabaseManager()


@app.get("/")
async def root():
    """Root endpoint with system information."""
    return {
        "message": "LLM Document Query Processing System",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs",
        "performance_optimized": True,
        "target_latency": "<20ms"
    }
    

@app.post("/api/v1/hackrx/run", response_model=QueryResponse)
async def process_queries(
    request: QueryRequest,
    token: str = Depends(verify_token)
):
    """
    Main endpoint for processing document queries - Optimized for <20ms total latency
    Returns response in the exact format: {"answers": ["answer1", "answer2", ...]}
    """
    try:
        start_time = time.time()
        
        # Log essential request information
        logger.info(f"ðŸ“„ Processing request - Document: {request.documents}")
        logger.info(f"â“ Questions ({len(request.questions)}): {[q  for q in request.questions]}")
        
        # Instant cache check for entire request
        request_hash = hashlib.md5(f"{request.documents}_{str(request.questions)}".encode()).hexdigest()
        if request_hash in instant_cache:
            logger.info("âš¡ Instant cache hit - returning cached response")
            return QueryResponse(answers=instant_cache[request_hash])
        
        if request_hash in ultra_fast_cache:
            logger.info("ðŸš€ Ultra-fast cache hit - returning cached response")
            return QueryResponse(answers=ultra_fast_cache[request_hash])
        
        if request_hash in response_cache:
            logger.info("ðŸ’¾ Response cache hit - returning cached response")
            return QueryResponse(answers=response_cache[request_hash])
        
        # Step 0: Check if document already processed (cached check)
        cache_key = f"doc_{hash(request.documents)}"
        already_exists = cache_key in result_cache or await database_manager.document_exists(request.documents)
        
        if already_exists:
            logger.info("ðŸ“‹ Document already processed - skipping ingestion")
        else:
            logger.info("ðŸ”„ Document not found - starting async processing")
            # Step 1: Process and ingest document (async, non-blocking)
            asyncio.create_task(_process_document_async(request.documents))

        # Step 2: Process all queries with instant optimization
        answers = await _process_queries_instant(request.questions, request.documents)

        # Cache the entire response for instant future access
        instant_cache[request_hash] = answers
        ultra_fast_cache[request_hash] = answers
        response_cache[request_hash] = answers

        # Log processing time and success
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"âœ… Request completed successfully in {processing_time:.2f}ms")

        # Return response in exact format requested
        return QueryResponse(answers=answers)

    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ Request failed after {processing_time:.2f}ms - Error: {str(e)}")
        
        # Provide more specific error messages
        if "timeout" in str(e).lower():
            raise HTTPException(status_code=408, detail="Request timed out. Please try again.")
        elif "rate limit" in str(e).lower():
            raise HTTPException(status_code=429, detail="Service is busy. Please wait a moment and try again.")
        elif "authentication" in str(e).lower() or "api key" in str(e).lower():
            raise HTTPException(status_code=401, detail="Authentication error. Please check API configuration.")
        else:
            raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

async def _process_document_async(document_url: str):
    """Process document asynchronously without blocking the main response"""
    try:
        logger.info(f"ðŸ”„ Processing document: {document_url}")
        
        # Clear cache before processing new document to prevent stale results
        await vector_store.clear_cache()
        instant_cache.clear()
        logger.info(f"ðŸ§¹ Cache cleared before processing new document")
        
        document_chunks = await document_processor.process_document(document_url)
        await vector_store.add_documents(document_chunks)
        asyncio.create_task(
            database_manager.insert_document(document_url, document_chunks[0].metadata if document_chunks else {})
        )
        logger.info(f"âœ… Document processed successfully: {len(document_chunks)} chunks")
    except Exception as e:
        logger.error(f"âŒ Document processing failed: {str(e)}")

async def _process_queries_instant(questions: List[str], document_url: str) -> List[str]:
    """Process all queries with instant optimization for <20ms total latency"""
    try:
        # Instant parallel processing
        tasks = []
        for question in questions:
            # Check instant cache first
            question_hash = hashlib.md5(question.strip().encode()).hexdigest()
            if question_hash in instant_cache:
                tasks.append(asyncio.create_task(asyncio.sleep(0, result=instant_cache[question_hash])))
            else:
                task = _generate_answer_instant(question, document_url)
                tasks.append(task)
        
        answers = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle any exceptions
        processed_answers = []
        for i, answer in enumerate(answers):
            if isinstance(answer, Exception):
                logger.error(f"âŒ Question {i+1} failed: {str(answer)}")
                # Provide more specific error messages
                if "timeout" in str(answer).lower():
                    processed_answers.append("Request timed out. Please try again with a simpler question.")
                elif "rate limit" in str(answer).lower():
                    processed_answers.append("Service is busy. Please wait a moment and try again.")
                elif "authentication" in str(answer).lower() or "api key" in str(answer).lower():
                    processed_answers.append("Authentication error. Please check API configuration.")
                else:
                    processed_answers.append(f"Error processing this question: {str(answer)[:100]}...")
            else:
                processed_answers.append(answer)

        return processed_answers

    except Exception as e:
        logger.error(f"âŒ Query processing failed: {str(e)}")
        # Provide more specific error messages
        if "timeout" in str(e).lower():
            return ["Request timed out. Please try again with simpler questions."] * len(questions)
        elif "rate limit" in str(e).lower():
            return ["Service is busy. Please wait a moment and try again."] * len(questions)
        elif "authentication" in str(e).lower() or "api key" in str(e).lower():
            return ["Authentication error. Please check API configuration."] * len(questions)
        else:
            return [f"Error processing questions: {str(e)[:100]}..."] * len(questions)

async def _generate_answer_instant(question: str, document_url: str) -> str:
    """Generate answer with instant optimization for <20ms total latency"""
    try:
        # Check instant cache
        question_hash = hashlib.md5(question.strip().encode()).hexdigest()
        if question_hash in instant_cache:
            return instant_cache[question_hash]
        
        # Generate embedding with instant caching
        embedding = await query_engine.generate_query_embedding(question)
        
        # Perform enhanced similarity search with instant caching
        relevant_chunks = await vector_store.similarity_search(embedding, top_k=8, query_text=question, document_url=document_url)  # Enhanced search with document caching
        
        # Prepare context from relevant chunks (use multiple chunks for maximum context coverage)
        context = ""
        for i, chunk in enumerate(relevant_chunks):  # Use up to 8 chunks
            context += f"Context {i+1}: {chunk['content']}\n\n"  # Increased from 300 to 800 chars per chunk
        
        if not context.strip():
            context = "No relevant information found in the documents."
        
        # Generate answer with instant caching
        answer = await query_engine.generate_answer(question, relevant_chunks, document_url)
        
        # Cache the answer
        instant_cache[question_hash] = answer
        return answer
        
    except Exception as e:
        logger.error(f"âŒ Answer generation failed for question '{question}...': {str(e)}")
        
        # Provide more specific error messages
        if "timeout" in str(e).lower():
            return "Request timed out. Please try again with a simpler question."
        elif "rate limit" in str(e).lower():
            return "Service is busy. Please wait a moment and try again."
        elif "authentication" in str(e).lower() or "api key" in str(e).lower():
            return "Authentication error. Please check API configuration."
        else:
            return f"Error processing question: {str(e)[:100]}..."

@app.get("/health")
async def health_check():
    """Health check endpoint with performance metrics"""
    system_stats = performance_monitor.get_system_stats()
    return {
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "system_stats": system_stats,
        "performance_metrics": {
            "embedding_generation_avg": performance_monitor.get_average_time("embedding_generation"),
            "similarity_search_avg": performance_monitor.get_average_time("similarity_search"),
            "answer_generation_avg": performance_monitor.get_average_time("answer_generation"),
            "target_latency": "<10s"
        }
    }

@app.post("/api/v1/hackrx/clear-cache")
async def clear_cache(token: str = Depends(verify_token)):
    """Clear all caches to prevent stale results"""
    try:
        await vector_store.clear_cache()
        # Also clear the instant cache in main.py
        instant_cache.clear()
        return {"message": "All caches cleared successfully", "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

@app.get("/api/v1/hackrx/cache-stats")
async def get_cache_stats(token: str = Depends(verify_token)):
    """Get cache statistics for monitoring"""
    try:
        vector_cache_stats = await vector_store.get_cache_stats()
        return {
            "vector_store_cache": vector_cache_stats,
            "instant_cache_size": len(instant_cache),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting cache stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get cache stats: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
